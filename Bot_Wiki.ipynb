{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTDm7LrtscWJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDhZ6SIjs3bG"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()\n",
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "model.to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB7NDbJHuI9b"
      },
      "outputs": [],
      "source": [
        "!pip install pyTelegramBotApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iFfCOSx4xvM-"
      },
      "outputs": [],
      "source": [
        "import telebot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_cTd33ARv10x"
      },
      "outputs": [],
      "source": [
        "def generate(text,stop=18,limit=True,temp=0.15,max=40,k=0,p=0.95,rp=1.0,num=1,no_repeat_ngram_size=2,num_beams=1,early_stopping=False,length_penalty=1):\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    greedy_output = model.generate(input_ids, max_length=len(input_ids[0])+max, temperature=temp, num_beams=num_beams,early_stopping=early_stopping, top_k=k, top_p=p, no_repeat_ngram_size=no_repeat_ngram_size, num_return_sequences=num,do_sample=True, repetition_penalty=rp,eos_token_id=stop,length_penalty=length_penalty)\n",
        "    if num > 1:\n",
        "        outs = []\n",
        "        for out in greedy_output:\n",
        "            outs.append(tokenizer.decode(out, skip_special_tokens=True))\n",
        "        return outs\n",
        "    return tokenizer.decode(greedy_output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hEkWtw0rs6xN"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Bot-wiki\"\"\"\n",
        "\n",
        "history = {}\n",
        "\n",
        "def get_answer(question,id):\n",
        "  question = question.capitalize()\n",
        "  if not question[-1] in [\"?\",\"!\",\".\"]:\n",
        "    question = question + \".\"\n",
        "  gen = generate(prompt.format(question))\n",
        "  ans = gen.replace(prompt.format(question),\"\")\n",
        "  print(gen)\n",
        "  if ans[0] == \" \":\n",
        "    ans = ans[1:]\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EV1CCNGe0PcR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_generated(prompt,gen):\n",
        "    gen = gen.replace(prompt,'')\n",
        "    tmp = gen.split('\\n')\n",
        "    find = re.findall(r'(.*?\\?)', tmp[0])\n",
        "    if len(find) > 1:\n",
        "        tmp[0] = find[0]\n",
        "    gen = tmp[0]\n",
        "    prompt = prompt + gen\n",
        "    return prompt,gen\n",
        "def clean_before(prompt):\n",
        "    while str.find(prompt,\" .\") != -1:\n",
        "        prompt = prompt.replace(\" .\",'.')\n",
        "    while str.find(prompt,\" !\") != -1:\n",
        "        prompt = prompt.replace(\" !\",'!')\n",
        "    while str.find(prompt,\" ?\") != -1:\n",
        "        prompt = prompt.replace(\" ?\",'?')\n",
        "    while str.find(prompt,\" ,\") != -1:\n",
        "        prompt = prompt.replace(\" ,\",',')\n",
        "    return prompt\n",
        "class Brain:\n",
        "    def __init__(self,name,prompt=\"\",memory=[]):\n",
        "        self.name = name\n",
        "        self.prompt = prompt\n",
        "        self.memory = memory\n",
        "    def say_to_brain(self,who,text):\n",
        "        ttmp = list(text)\n",
        "        ttmp[0] = ttmp[0].capitalize()\n",
        "        if not ttmp[-1] in '.?!,':\n",
        "            ttmp.append(\".\")\n",
        "\n",
        "        text = \"\".join(ttmp)\n",
        "\n",
        "        self.memory.append(\"{}: {}\".format(who,text))\n",
        "\n",
        "        textmem = \"\\n\\n\".join(self.memory)\n",
        "\n",
        "        textmem += \"\\n\\n{}: \".format(self.name)\n",
        "        memtok = tokenizer.encode( self.prompt + textmem, return_tensors='pt' )\n",
        "        if len(memtok[0]) > 1900:\n",
        "            tmpmem = textmem\n",
        "            tmp = tokenizer.encode( self.prompt + tmpmem, return_tensors='pt' )\n",
        "            while len(tmp[0]) > 1900:\n",
        "                tmpmem = tmpmem[100:]\n",
        "                tmp = tokenizer.encode( self.prompt + tmpmem, return_tensors='pt' )\n",
        "            textmem = tmpmem\n",
        "            \n",
        "        textmem = clean_before(textmem)\n",
        "        gen = generate(self.prompt + textmem,num_beams=1,early_stopping=True,no_repeat_ngram_size=2,temp=0.5,k=40,p=0.92)\n",
        "\n",
        "        p,g = clean_generated(self.prompt + textmem, gen)\n",
        "        self.memory.append(\"{}: {}\".format(self.name,g))\n",
        "        return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Dv9OAwxpfl",
        "outputId": "da6a71c2-3d3b-4af6-fd4b-aeb4e6d06c2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 18 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: 123\n",
            "A: Задачей программиста является написание программного обеспечения под конкретную задачу пользователя путём создания программной реализации алгоритма работы программы средствами языка программирования высокого уровня абстракции ЛИНЕЙНОГО ИНТЕЛЛЕК\n"
          ]
        }
      ],
      "source": [
        "bot = telebot.TeleBot(token=\"6006315509:AAF3PWQgVq35OgA1H1Ob9qiFCu6InBKU4f0\")\n",
        "\n",
        "bots = {}\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start(message):\n",
        "  global bots\n",
        "  bots[message.from_user.id] = Brain(\"Википедия\",\"Википедия - чат-бот. Википедия - виртуальный собеседник. Википедия - виртуальный цифровой помощник.\\n\\n\")\n",
        "  bot.send_message(message.chat.id, \"Какие у тебя есть вопросы ?\")\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def text(message):\n",
        "  global bots\n",
        "  if not message.from_user.id in bots:\n",
        "    bots[message.from_user.id] = Brain(\"Википедия\",\"Википедия - чат-бот. Википедия - виртуальный собеседник. Википедия - виртуальный цифровой помощник.\\n\\n\")\n",
        "  ans = bots[message.chat.id].say_to_brain(\"Человек\",message.text)\n",
        "  print(f\"Q: {message.text}\\nA: {ans}\")\n",
        "  bot.send_message(message.chat.id, ans)\n",
        "\n",
        "bot.polling(none_stop=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
